{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b74378-c237-4870-b63b-0a4af9b1fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23128353-099e-431b-adcb-dac428d3f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "2.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example is based on instructions from \n",
    "# https://github.com/kach/gradient-descent-the-ultimate-optimizer, \n",
    "# which contains the original source code for the Gradient Descent: The Ultimate Optimizer paper,\n",
    "# and the modified source code for experimental results with MNIST dataset.\n",
    "\n",
    "class MNIST_FullyConnected(nn.Module):\n",
    "    def __init__(self, num_inp, num_hid, num_out):\n",
    "        super(MNIST_FullyConnected, self).__init__()\n",
    "        self.layer1 = nn.Linear(num_inp, num_hid)\n",
    "        self.layer2 = nn.Linear(num_hid, num_out)\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.kaiming_uniform_(self.layer1.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.layer2.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 30\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "dl_train = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "dl_test = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False)\n",
    "\n",
    "model = MNIST_FullyConnected(28 * 28, 128, 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebe4e51-311d-4ae5-8e56-f30978b0cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes the module wrapper with respective optimizer stack\n",
    "\n",
    "# Commented optimizers correspond to the optimizers used to obtain the results.\n",
    "# Optimizers in which clip is set to True have gradient clipping enabled.\n",
    "# Gradient clipping is disabled by default.\n",
    "\n",
    "import gdtuo_gradient_clipping as gdtuo\n",
    "\n",
    " # SGD / SGD(alpha = 0.01) with no gradient clipping for second SGD\n",
    "gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.01))\n",
    "\n",
    "# SGD / SGD(alpha = 0.01) with gradient clipping for second SGD\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.01, clip=True))\n",
    "\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.05))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.05, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.1))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.5))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=0.5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=1))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=5))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=10))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=10, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=50))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=50, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=75)\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=75, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=100))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=100, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=100))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=100, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=250))\n",
    "# gdtuo_optimizer = gdtuo.SGD(optimizer=gdtuo.SGD(alpha=250, clip=True))\n",
    "\n",
    "# Adam / SGD(alpha = 10^-5) with no gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-5))\n",
    "\n",
    "# Adam / SGD(alpha = 10^-5) with gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-5, clip=True))\n",
    "\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-5))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-4))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-4, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-4))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-4, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-3))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-3, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-3))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-3, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-2))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-2))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-1))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=1e-1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-1))\n",
    "# gdtuo_optimizer = gdtuo.Adam(optimizer=gdtuo.SGD(alpha=5e-1, clip=True))\n",
    "\n",
    "# AdaGrad / SGD(alpha = 10^-5) with no gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.01))\n",
    "\n",
    "# AdaGrad / SGD(alpha = 10^-5) with gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.01, clip=True))\n",
    "\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.05))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.05, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.1))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.5))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=0.5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=1))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=2.5))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=2.5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=5))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=7.5))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=7.5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=10))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=10, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=50))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=50, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=100))\n",
    "# gdtuo_optimizer = gdtuo.AdaGrad(optimizer=gdtuo.SGD(alpha=100, clip=True))\n",
    "\n",
    "# RMSProp / SGD(alpha = 10^-5) with no gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-5))\n",
    "\n",
    "# RMSProp / SGD(alpha = 10^-5) with gradient clipping for SGD as secondary optimizer\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-5, clip=True))\n",
    "\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-5))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-5, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-4))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-4, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-4))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-4, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-3))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-3, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-3))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-3, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-2))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(2.5e-2))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(2.5e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-2))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(7.5e-2))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(7.5e-2, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-1))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(1e-1, clip=True))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-1))\n",
    "# gdtuo_optimizer = gdtuo.RMSProp(optimizer=gdtuo.SGD(5e-1, clip=True))\n",
    "\n",
    "# Creates module wrapper with optimizer stack functionality\n",
    "mw = gdtuo.ModuleWrapper(model, optimizer=gdtuo_optimizer)\n",
    "mw.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0b968a-32d1-408f-8af9-6e6fcbf93830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dl_train, num_epochs):\n",
    "    print()\n",
    "    print_model_optimizer_parameters(model)\n",
    "    for i in range(1, num_epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        for j, (features_, labels_) in enumerate(dl_train):\n",
    "            model.begin() # before each step, enables gradient tracking on desired parameters\n",
    "            features, labels = torch.reshape(features_, (-1, 28 * 28)).to(device), labels_.to(device)\n",
    "            prediction = model.forward(features)\n",
    "            loss = F.nll_loss(prediction, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward(create_graph=True)\n",
    "            model.step()\n",
    "            total_loss += loss.item() * features_.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(dl_train.dataset)\n",
    "        print(\"\\nepoch: {}, train loss: {}\".format(i, train_loss))\n",
    "        print_model_optimizer_parameters(model)\n",
    "\n",
    "# Function to output the model's primary optimizer's hyperparameter values\n",
    "# For example, if the stack was Adam / SGD, it would output the hyperparameter values of Adam\n",
    "def print_model_optimizer_parameters(model):\n",
    "    optimizer = model.optimizer\n",
    "    if isinstance(optimizer, gdtuo.NoOpOptimizer):\n",
    "        print(\"No optimizer passed into gdtuo model.\")\n",
    "        return\n",
    "\n",
    "    optimizer_type = \"unknown\"\n",
    "    if isinstance(optimizer, gdtuo.SGD):\n",
    "        optimizer_type = \"SGD\"\n",
    "    elif isinstance(optimizer, gdtuo.SGDPerParam):\n",
    "        optimizer_type = \"SGDPerParam\"\n",
    "    elif isinstance(optimizer, gdtuo.AdaGrad):\n",
    "        optimizer_type = \"AdaGrad\"\n",
    "    elif isinstance(optimizer, gdtuo.RMSProp):\n",
    "        optimizer_type = \"RMSProp\"\n",
    "    elif isinstance(optimizer, gdtuo.RMSPropAlpha):\n",
    "        optimizer_type = \"RMSPropAlpha\"\n",
    "    elif isinstance(optimizer, gdtuo.Adam):\n",
    "        optimizer_type = \"Adam\"\n",
    "    elif isinstance(optimizer, gdtuo.AdamBaydin):\n",
    "        optimizer_type = \"AdamBaydin\"\n",
    "    \n",
    "    print(\"{} optimizer parameters:\".format(optimizer_type))\n",
    "    \n",
    "    optimizer_parameters = optimizer.parameters\n",
    "\n",
    "    for parameter in optimizer_parameters:\n",
    "        value = optimizer_parameters[parameter]\n",
    "        if parameter == \"alpha\" and \\\n",
    "        (isinstance(optimizer, gdtuo.RMSProp) or isinstance(optimizer, gdtuo.RMSPropAlpha)):\n",
    "            value = torch.square(value)\n",
    "        if parameter == \"beta1\" or parameter == \"beta2\":\n",
    "            value = gdtuo.Adam.clamp(value)\n",
    "        if parameter == \"gamma\":\n",
    "            value = gdtuo.RMSProp.clamp(value)\n",
    "        print(\"{}: {}\\t\".format(parameter, value), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07db35e9-5878-487f-aa70-b7b2dab37245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.009999999776482582\tmu: 0.0\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\fall2023\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\engine.cpp:1156.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1, train loss: 1.3784865905125936\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.1438463181257248\tmu: 0.0\t\n",
      "\n",
      "epoch: 2, train loss: 1.0674078020095825\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.13962674140930176\tmu: 0.0\t\n",
      "\n",
      "epoch: 3, train loss: 1.022485931332906\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.13056053221225739\tmu: 0.0\t\n",
      "\n",
      "epoch: 4, train loss: 1.0005578675270081\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.12352947145700455\tmu: 0.0\t\n",
      "\n",
      "epoch: 5, train loss: 0.9862431994438171\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.11825825273990631\tmu: 0.0\t\n",
      "\n",
      "epoch: 6, train loss: 0.9753477774937948\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.11621797829866409\tmu: 0.0\t\n",
      "\n",
      "epoch: 7, train loss: 0.9663915136655171\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.11194553971290588\tmu: 0.0\t\n",
      "\n",
      "epoch: 8, train loss: 0.9586406548182169\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.10790853947401047\tmu: 0.0\t\n",
      "\n",
      "epoch: 9, train loss: 0.9520755769411723\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.1033453419804573\tmu: 0.0\t\n",
      "\n",
      "epoch: 10, train loss: 0.9462834394772848\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.10002029687166214\tmu: 0.0\t\n",
      "\n",
      "epoch: 11, train loss: 0.9411608518282573\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.09722122550010681\tmu: 0.0\t\n",
      "\n",
      "epoch: 12, train loss: 0.9365209731737772\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.09437259286642075\tmu: 0.0\t\n",
      "\n",
      "epoch: 13, train loss: 0.9324292146364848\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.09280919283628464\tmu: 0.0\t\n",
      "\n",
      "epoch: 14, train loss: 0.9285583651542664\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.08965956419706345\tmu: 0.0\t\n",
      "\n",
      "epoch: 15, train loss: 0.9251924905459086\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0884748324751854\tmu: 0.0\t\n",
      "\n",
      "epoch: 16, train loss: 0.9220430036226909\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.08539973944425583\tmu: 0.0\t\n",
      "\n",
      "epoch: 17, train loss: 0.9191197754224142\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0846320241689682\tmu: 0.0\t\n",
      "\n",
      "epoch: 18, train loss: 0.9164456636110941\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.08274714648723602\tmu: 0.0\t\n",
      "\n",
      "epoch: 19, train loss: 0.9139299971262614\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0808432325720787\tmu: 0.0\t\n",
      "\n",
      "epoch: 20, train loss: 0.9115676622708638\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.07928414642810822\tmu: 0.0\t\n",
      "\n",
      "epoch: 21, train loss: 0.9094578523953756\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.07824460417032242\tmu: 0.0\t\n",
      "\n",
      "epoch: 22, train loss: 0.9073722277641296\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.07720980793237686\tmu: 0.0\t\n",
      "\n",
      "epoch: 23, train loss: 0.9053815158843994\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.07656697928905487\tmu: 0.0\t\n",
      "\n",
      "epoch: 24, train loss: 0.9035814728101095\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0743086114525795\tmu: 0.0\t\n",
      "\n",
      "epoch: 25, train loss: 0.9018296417236328\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0723574087023735\tmu: 0.0\t\n",
      "\n",
      "epoch: 26, train loss: 0.9002210999806722\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.07132142782211304\tmu: 0.0\t\n",
      "\n",
      "epoch: 27, train loss: 0.8987298745791117\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.06954673677682877\tmu: 0.0\t\n",
      "\n",
      "epoch: 28, train loss: 0.8972660859743754\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.06906596571207047\tmu: 0.0\t\n",
      "\n",
      "epoch: 29, train loss: 0.8958671866099039\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.0676765963435173\tmu: 0.0\t\n",
      "\n",
      "epoch: 30, train loss: 0.894586809762319\n",
      "SGD optimizer parameters:\n",
      "alpha: 0.06629284471273422\tmu: 0.0\t\n"
     ]
    }
   ],
   "source": [
    "train_model(mw, dl_train, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e3a6fe-63ee-4dd7-9882-3f0e1cbe9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_error(model, dl_data):\n",
    "    num_correct_classifications = 0\n",
    "    num_datapoints = len(dl_data.dataset)\n",
    "    for j, (features_, labels_) in enumerate(dl_data):\n",
    "        features, labels = torch.reshape(features_, (-1, 28 * 28)).to(device), labels_.to(device)\n",
    "        prediction = model.forward(features)\n",
    "        for i, row_in_prediction in enumerate(prediction):\n",
    "            predicted_label = torch.argmax(row_in_prediction)\n",
    "            if predicted_label == labels[i]:\n",
    "                num_correct_classifications += 1\n",
    "\n",
    "    accuracy = num_correct_classifications / num_datapoints\n",
    "    error = 1 - accuracy\n",
    "    \n",
    "    return accuracy, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1459e200-486e-45ef-b77b-e8267bf858f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results from model\n",
      "train accuracy: 95.325%\n",
      "train error: 4.674999999999995%\n",
      "test accuracy: 95.04%\n",
      "test error: 4.959999999999997%\n",
      "\n",
      "sgd / sgd / parameter gradient info:\n",
      "parameter alpha:\n",
      "\tmaximum gradient norm: 0.4152931272983551\n",
      "\taverage gradient norm: 0.006751393433660269\n",
      "parameter mu:\n",
      "\tmaximum gradient norm: 0.0\n",
      "\taverage gradient norm: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"results from model\")\n",
    "\n",
    "# outputs accuracy of the model with the corresponding optimizer stack\n",
    "\n",
    "train_accuracy, train_error = get_accuracy_error(mw, dl_train)\n",
    "print(\"train accuracy: {}%\".format(train_accuracy * 100))\n",
    "print(\"train error: {}%\".format(train_error * 100))\n",
    "\n",
    "test_accuracy, test_error = get_accuracy_error(mw, dl_test)\n",
    "print(\"test accuracy: {}%\".format(test_accuracy * 100))\n",
    "print(\"test error: {}%\".format(test_error * 100))\n",
    "\n",
    "print()\n",
    "\n",
    "# outputs gradient information of the primary optimizer's parameters\n",
    "mw.optimizer.print_parameter_gradient_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
